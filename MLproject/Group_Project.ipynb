{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import csv, time\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify output data for post- or pre- experiment\n",
    "\n",
    "def get_flour(option):    # option: pre or post\n",
    "\n",
    "    file = pd.read_csv('./Outputs/FlourishingScale.csv')\n",
    "    # read data from csv\n",
    "    n = file.to_numpy()\n",
    "\n",
    "    # we only use pre as our output\n",
    "    n = n[n[:, 1] == option]\n",
    "\n",
    "    # convert the id of the student into integer\n",
    "    n[:, 0] = [int(i[1:]) for i in n[:, 0]]\n",
    "\n",
    "    # get rid of 'pre' column\n",
    "    n = n[:, np.delete(np.arange(10), 1)]\n",
    "\n",
    "    # find all nan value and change them into -1\n",
    "    n[pd.isnull(n)] = -1\n",
    "\n",
    "    # delete -1(nan) row\n",
    "    for i in range(1, 9):\n",
    "        n = np.delete(n, np.where(n[:, i] == -1), axis=0)\n",
    "\n",
    "    result = np.zeros((n.shape[0], 2))\n",
    "    result[:, 0] = n[:, 0]\n",
    "    result[:, 1] = n[:, 1:].sum(axis=1)\n",
    "\n",
    "    # use median as our threshold\n",
    "    a = np.median(result[:, 1])\n",
    "    result[:, 1][result[:, 1] <= a] = 0\n",
    "    result[:, 1][result[:, 1] > a] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_panas(option):    # option: pre or post\n",
    "\n",
    "    global pos_sum, neg_sum\n",
    "    output = pd.read_csv('./Outputs/panas.csv')\n",
    "    n = output.to_numpy()\n",
    "\n",
    "    data = n[n[:, 1] == option][:, 2:]\n",
    "    index = np.array([int(i[1:]) for i in n[n[:, 1] == option][:, 0]]).reshape(-1, 1)\n",
    "\n",
    "    data[pd.isnull(data)] = 0\n",
    "\n",
    "    data = data.astype(int)\n",
    "    data = np.concatenate((index, data), axis=1)\n",
    "\n",
    "    for i in range(1, 18):\n",
    "        data = np.delete(data, np.where(data[:, i] == 0), axis=0)\n",
    "\n",
    "    index = data[:, 0].reshape(-1, 1)\n",
    "    data = data[:, 1:]\n",
    "    positive = ['Interested', 'Strong', 'Enthusiastic', 'Proud', 'Alert', 'Inspired', 'Determined', 'Attentive',\n",
    "                'Active']\n",
    "    negative = ['Distressed', 'Upset', 'Guilty', 'Scared', 'Hostile', 'Irritable', 'Nervous', 'Jittery', 'Afraid']\n",
    "\n",
    "    summ = np.zeros(data.shape[0])\n",
    "    pos_sum = np.zeros(data.shape[0])\n",
    "    neg_sum = np.zeros(data.shape[0])\n",
    "\n",
    "    for i in range(9):\n",
    "        ind_pos = [i.strip() for i in output.keys()[2:]].index(positive[i])\n",
    "        ind_neg = [i.strip() for i in output.keys()[2:]].index(negative[i])\n",
    "        summ += data[:, ind_pos]\n",
    "        summ -= data[:, ind_neg]\n",
    "        pos_sum += data[:, ind_pos]\n",
    "        neg_sum += data[:, ind_neg]\n",
    "\n",
    "\n",
    "    pos_sum = pos_sum.reshape(-1,1)\n",
    "    neg_sum = neg_sum.reshape(-1,1)\n",
    "    p_m, n_m = pos_sum.mean(), neg_sum.mean()\n",
    "\n",
    "    pos_sum[pos_sum[:,0] <= p_m] = 0\n",
    "    pos_sum[pos_sum[:,0] > p_m] = 1\n",
    "\n",
    "    neg_sum[neg_sum[:,0] <= n_m] = 0  \n",
    "    neg_sum[neg_sum[:,0] > n_m] = 1\n",
    "\n",
    "    return np.concatenate((index,pos_sum), axis = 1), np.concatenate((index, neg_sum), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification for the mean value of pre and post\n",
    "\n",
    "def get_flour_mean():\n",
    "    \n",
    "    file = pd.read_csv('./Outputs/FlourishingScale.csv')\n",
    "    # read data from csv\n",
    "    n = file.to_numpy()\n",
    "\n",
    "    # we only use pre as our output\n",
    "    n_pre = n[n[:, 1] == 'pre']\n",
    "    n_post = n[n[:, 1] == 'post']\n",
    "\n",
    "    # convert the id of the student into integer\n",
    "    n_pre[:, 0] = [int(i[1:]) for i in n_pre[:, 0]]\n",
    "    n_post[:, 0] = [int(i[1:]) for i in n_post[:, 0]]\n",
    "\n",
    "    # get rid of 'pre' column\n",
    "    n_pre = n_pre[:, np.delete(np.arange(10), 1)]\n",
    "    n_post = n_post[:, np.delete(np.arange(10), 1)]\n",
    "\n",
    "    # find all nan value and change them into -1\n",
    "    n_pre[pd.isnull(n_pre)] = -1\n",
    "    n_post[pd.isnull(n_post)] = -1\n",
    "\n",
    "    # delete -1(nan) row\n",
    "    for i in range(1, 9):\n",
    "        n_pre = np.delete(n_pre, np.where(n_pre[:, i] == -1), axis=0)\n",
    "        n_post = np.delete(n_post, np.where(n_post[:, i] == -1), axis=0)\n",
    "    \n",
    "    new_pre = n_pre[np.isin(n_pre[:,0], n_post[:,0])]\n",
    "    new_post = n_post[np.isin(n_post[:,0], n_pre[:,0])]\n",
    "    new_pre_post = (new_pre[:,1] + new_post[:,1])/2\n",
    "    \n",
    "    new = np.concatenate((new_pre[:,0].reshape(-1,1), new_pre_post.reshape(-1,1)),axis = 1)\n",
    "\n",
    "    result = np.zeros((new.shape[0], 2))\n",
    "    result[:, 0] = new[:, 0]\n",
    "    result[:, 1] = new[:, 1:].sum(axis=1)\n",
    "\n",
    "    # use median as our threshold\n",
    "    a = np.median(result[:, 1])\n",
    "    result[:, 1][result[:, 1] <= a] = 0\n",
    "    result[:, 1][result[:, 1] > a] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "# both pre and post\n",
    "def get_panas_mean():   \n",
    "    def get_panas(option):\n",
    "\n",
    "        output = pd.read_csv('./Outputs/panas.csv')\n",
    "        n = output.to_numpy()\n",
    "\n",
    "        data = n[n[:, 1] == option][:, 2:]\n",
    "        index = np.array([int(i[1:]) for i in n[n[:, 1] == option][:, 0]]).reshape(-1, 1)\n",
    "\n",
    "        data[pd.isnull(data)] = 0\n",
    "       \n",
    "\n",
    "        data = data.astype(int)\n",
    "        data = np.concatenate((index, data), axis=1)\n",
    "\n",
    "        for i in range(1, 18):\n",
    "            data = np.delete(data, np.where(data[:, i] == 0), axis=0)\n",
    "\n",
    "        index = data[:, 0].reshape(-1, 1)\n",
    "        data = data[:, 1:]\n",
    "        positive = ['Interested', 'Strong', 'Enthusiastic', 'Proud', 'Alert', 'Inspired', 'Determined', 'Attentive',\n",
    "                    'Active']\n",
    "        negative = ['Distressed', 'Upset', 'Guilty', 'Scared', 'Hostile', 'Irritable', 'Nervous', 'Jittery', 'Afraid']\n",
    "\n",
    "        summ = np.zeros(data.shape[0])\n",
    "        pos_sum = np.zeros(data.shape[0])\n",
    "        neg_sum = np.zeros(data.shape[0])\n",
    "\n",
    "        for i in range(9):\n",
    "            ind_pos = [i.strip() for i in output.keys()[2:]].index(positive[i])\n",
    "            ind_neg = [i.strip() for i in output.keys()[2:]].index(negative[i])\n",
    "            summ += data[:, ind_pos]\n",
    "            summ -= data[:, ind_neg]\n",
    "            pos_sum += data[:, ind_pos]\n",
    "            neg_sum += data[:, ind_neg]\n",
    "\n",
    "        pos_sum = pos_sum.reshape(-1,1)\n",
    "        neg_sum = neg_sum.reshape(-1,1)\n",
    "        # return the really value\n",
    "        return np.concatenate((index,pos_sum), axis = 1), np.concatenate((index, neg_sum), axis = 1)\n",
    "\n",
    "    label_pre_pos, label_pre_neg = get_panas('pre')\n",
    "    label_post_pos, label_post_neg = get_panas('post')\n",
    "    new_pos = (label_pre_pos[np.isin(label_pre_pos[:,0], label_post_pos[:,0])] +\\\n",
    "              label_post_pos[np.isin(label_post_pos[:,0], label_pre_pos[:,0])])/2\n",
    "    new_neg = (label_pre_neg[np.isin(label_pre_neg[:,0], label_post_neg[:,0])] +\\\n",
    "              label_post_neg[np.isin(label_post_neg[:,0], label_pre_neg[:,0])])/2\n",
    "    result_pos = np.zeros((new_pos.shape[0], 2))\n",
    "    result_neg = np.zeros((new_neg.shape[0], 2))\n",
    "    result_pos[:, 0] = new_pos[:, 0]\n",
    "    result_pos[:, 1] = new_pos[:, 1:].sum(axis=1)\n",
    "    result_neg[:, 0] = new_neg[:, 0]\n",
    "    result_neg[:, 1] = new_neg[:, 1:].sum(axis=1)\n",
    "\n",
    "    # use median as our threshold\n",
    "    a = np.median(result_pos[:, 1])\n",
    "    b = np.median(result_neg[:, 1])\n",
    "    result_pos[:, 1][result_pos[:, 1] <= a] = 0\n",
    "    result_pos[:, 1][result_pos[:, 1] > a] = 1\n",
    "    result_neg[:, 1][result_neg[:, 1] <= b] = 0\n",
    "    result_neg[:, 1][result_neg[:, 1] > b] = 1\n",
    "    \n",
    "    return result_pos, result_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into 10 weeks\n",
    "\n",
    "def get_data(data_name, label): # for computing duration\n",
    "    total_data = []\n",
    "    min_value = float('inf')\n",
    "    max_value = 0\n",
    "\n",
    "    for i in range(60): # extract the valid student id from the file\n",
    "        try:\n",
    "            input = pd.read_csv(f'./inputs/sensing/{data_name}/{data_name}_u{i:02d}.csv').to_numpy()\n",
    "            total_data.append(i)\n",
    "            start_min = np.min(input[:, 0])\n",
    "            start_max = np.max(input[:, 0])\n",
    "            if start_min < min_value:\n",
    "                min_value = start_min\n",
    "            if start_max > max_value:\n",
    "                max_value = start_max\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    conversation_id = np.array(total_data).reshape(-1, 1)\n",
    "    partition = np.arange(min_value, max_value, 7 * 24 * 3600) # split into 10 weeks\n",
    "\n",
    "    conversation_data = np.zeros((len(total_data), len(partition)))\n",
    "\n",
    "    for i in range(len(total_data)):\n",
    "\n",
    "        student_id = total_data[i]\n",
    "\n",
    "        input = pd.read_csv(f'./inputs/sensing/{data_name}/{data_name}_u{student_id:02d}.csv').to_numpy()\n",
    "\n",
    "        week_sum = np.zeros(len(partition))\n",
    "\n",
    "        for j in range(len(partition) - 1): # search for data in corresponding time\n",
    "            data = input[np.where((input[:, 0] > partition[j]) * (input[:, 0] < partition[j + 1]))]\n",
    "            data_sum = sum(data[:, 1] - data[:, 0])\n",
    "            week_sum[j] = data_sum\n",
    "        data = input[np.where(input[:, 0] > partition[-1])]\n",
    "        week_sum[-1] = sum(data[:, 1] - data[:, 0])\n",
    "        # print(student_id, week_sum)\n",
    "        conversation_data[i] = week_sum\n",
    "        \n",
    "    # add student id\n",
    "    concatencate_data = np.concatenate((conversation_id, conversation_data), axis=1).astype(int)\n",
    "\n",
    "    new = np.zeros_like(concatencate_data) # add label\n",
    "    for i in range(len(label[:, 0])):\n",
    "        if label[i, 0] in concatencate_data[:, 0]:\n",
    "            index = np.where(concatencate_data[:, 0] == label[i, 0])[0]\n",
    "            new[i, :] = concatencate_data[index, :]\n",
    "    i = 48\n",
    "    while new[i, 0] == 0:\n",
    "        new = np.delete(new, -1, 0)\n",
    "        i -= 1\n",
    "    new = np.concatenate((new, label[:, 1].reshape(-1, 1)), axis=1).astype(int)\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def group_split(data_name, score, label): # for computing frequency\n",
    "    student_list = []\n",
    "    max_value = 0\n",
    "    min_value = float('inf')\n",
    "    for i in range(60):\n",
    "        try:\n",
    "            # read file\n",
    "            input = pd.read_csv(f'./inputs/sensing/{data_name}/{data_name}_u{i:02d}.csv').to_numpy()\n",
    "            #             print(input)\n",
    "            student_list.append(i)\n",
    "            min_v = np.min(input[0, 0])\n",
    "            max_v = np.max(input[-1, 0])\n",
    "            if max_v > max_value:\n",
    "                max_value = max_v\n",
    "            if min_v < min_value:\n",
    "                min_value = min_v\n",
    "        except IOError:\n",
    "            continue\n",
    "    student_id = np.array(student_list).reshape(-1, 1)\n",
    "    process_data = np.zeros((len(student_list), 10))\n",
    "    partition = np.arange(min_value, max_value, 7 * 24 * 3600)\n",
    "    for i in range(len(student_list)):\n",
    "        input = pd.read_csv(f'./inputs/sensing/{data_name}/{data_name}_u{student_list[i]:02d}.csv').to_numpy()\n",
    "        result = np.zeros(len(partition))\n",
    "\n",
    "        for j in range(len(partition) - 1):\n",
    "            data = input[np.where((input[:, 0] >= partition[j]) *\n",
    "                                  (input[:, 0] < partition[j + 1]) * (input[:, 1] == score))].shape[0]\n",
    "            result[j] = data * 2\n",
    "        result[-1] = input[np.where((input[:, 0] >= partition[-1]) * (input[:, 1] == score))].shape[0] * 2\n",
    "        process_data[i] = result\n",
    "\n",
    "    concatencate_data = np.concatenate((student_id, process_data), axis=1).astype(int)\n",
    "\n",
    "    new = np.zeros_like(concatencate_data)\n",
    "    for i in range(len(label[:, 0])):\n",
    "        if label[i, 0] in concatencate_data[:, 0]:\n",
    "            index = np.where(concatencate_data[:, 0] == label[i, 0])[0]\n",
    "            new[i, :] = concatencate_data[index, :]\n",
    "    i = 48\n",
    "    while new[i, 0] == 0:\n",
    "        new = np.delete(new, -1, 0)\n",
    "        i -= 1\n",
    "    new = np.concatenate((new, label[:, 1].reshape(-1, 1)), axis=1).astype(int)\n",
    "\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# night duration\n",
    "def transform_all(time_stamp): # transform timestamp to local time\n",
    "    utc_time = datetime.utcfromtimestamp(time_stamp)\n",
    "    time = utc_time + timedelta(hours = -4)\n",
    "    return time.day, time.hour, time.minute, time.second\n",
    "    \n",
    "class timegap:\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.attr = {}\n",
    "        self.compute()\n",
    "        self.g = self.gap()\n",
    "    def compute(self):\n",
    "        self.attr['start_date'] = self.a[0]\n",
    "        self.attr['end_date'] = self.b[0]\n",
    "        self.attr['start_hour'] = self.a[1]\n",
    "        self.attr['end_hour'] = self.b[1]\n",
    "        self.attr['start_min'] = self.a[2]\n",
    "        self.attr['end_min'] = self.b[2]\n",
    "        self.attr['start_sec'] = self.a[3]\n",
    "        self.attr['end_sec'] = self.a[3]\n",
    "    def gap(self):\n",
    "        g = float('-inf')\n",
    "        if self.attr['start_date'] == self.attr['end_date']: # same day\n",
    "            start = self.attr['start_hour'] * 3600 + self.attr['start_min'] * 60 + self.attr['start_sec']\n",
    "            end = self.attr['end_hour'] * 3600 + self.attr['end_min'] * 60 + self.attr['end_sec']\n",
    "            if self.attr['start_hour'] >= 18: # if start after 6pm\n",
    "#                 print(1)\n",
    "                g = (end - start)//60\n",
    "            elif self.attr['start_hour'] >= 8: # if start before 6pm after 8am\n",
    "#                 print(2)\n",
    "                g = (end - 18 * 3600)//60 if self.attr['end_hour'] >= 18 else 0 # if end before 6pm then g = 0\n",
    "            elif self.attr['end_hour'] >= 8: # end after 8am\n",
    "#                 print(3)\n",
    "                g = (8 * 3600 - start)//60\n",
    "            else:\n",
    "#                 print(4)\n",
    "                g = (end - start) // 60\n",
    "\n",
    "        else: # different day\n",
    "            start = self.attr['start_hour'] * 3600 + self.attr['start_min'] * 60 + self.attr['start_sec']\n",
    "            end = self.attr['end_hour'] * 3600 + self.attr['end_min'] * 60 + self.attr['end_sec']\n",
    "            if self.attr['end_hour'] >= 8 and self.attr['start_hour'] >= 18: # if end after 8am, start after 6pm\n",
    "#                 print(5)\n",
    "                g = (8 * 3600 + (24 * 3600 - start))//60\n",
    "            elif self.attr['end_hour'] >= 8 and self.attr['start_hour'] < 18: # if end after 8am, start before 6pm\n",
    "#                 print(6)\n",
    "                g = (8 * 3600 + 6 * 3600)//60\n",
    "            elif self.attr['end_hour'] < 8 and self.attr['start_hour'] >= 18: # if end after 8am, start before 6pm\n",
    "#                 print(7)\n",
    "                g = (end + (24 * 3600 - start))//60\n",
    "            else:\n",
    "#                 print(8)\n",
    "                g = (end + 6 * 3600)//60\n",
    "        return g\n",
    "\n",
    "\n",
    "def dark_night(f): # extract night time\n",
    "    file = open(f)\n",
    "    x = np.array([i for i in csv.reader(file) if i][1:]).astype(int) \n",
    "\n",
    "    mid = 1362096000\n",
    "    while mid < x[0][0]:\n",
    "        mid += 24 * 3600\n",
    "    mid -= 24 * 3600\n",
    "\n",
    "    interval = [[]]\n",
    "\n",
    "    for i in x:\n",
    "#         print(interval)\n",
    "        if i[0] < (mid + 24 * 3600):\n",
    "            interval[-1].append(i)\n",
    "        else:\n",
    "            while True:\n",
    "                mid += 24 * 3600\n",
    "                if i[0] < (mid + 24 * 3600):\n",
    "                    interval.append([i])\n",
    "                    break\n",
    "                else:\n",
    "                    interval.append([np.array([0,0])])\n",
    "            \n",
    "    y = []\n",
    "    for i in x:\n",
    "        y.append([transform_all(i[0]),transform_all(i[1])])\n",
    "    #     print(time.ctime(i[0]),time.ctime(i[1]), sep = ' || ')\n",
    "#     print(y)\n",
    "    yi = [[]]\n",
    "    for i in interval:\n",
    "        for j in i:\n",
    "            yi[-1].append([transform_all(j[0]),transform_all(j[1])])\n",
    "        yi.append([])\n",
    "    yi.pop()\n",
    "\n",
    "    sleep_time = 0\n",
    "    sleep_day = []\n",
    "#     current_s = y[0][0].split()[2]\n",
    "\n",
    "    for itv in yi:\n",
    "        for i in itv:\n",
    "            tg = timegap(*i)\n",
    "    #         print(f\"{tg.g/60:.1f}\")\n",
    "    #         print(i)\n",
    "            sleep_time += tg.g\n",
    "        sleep_day.append(sleep_time)\n",
    "        sleep_time = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print(np.array(sleep_day))\n",
    "    return np.array(sleep_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darkNights = np.zeros(60)\n",
    "\n",
    "for i in range(60): # extract night time for dark dataset\n",
    "    f = f'./Inputs/sensing/dark/dark_u{i:02}.csv'\n",
    "    try:\n",
    "        \n",
    "        darkNights[i] = dark_night(f).sum()\n",
    "#         print(i, dark_night(f).mean())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "plock = np.zeros(60)\n",
    "\n",
    "for i in range(60):  # extract night time for phonelock dataset\n",
    "    f = f'./Inputs/sensing/phonelock/phonelock_u{i:02}.csv'\n",
    "    try:\n",
    "        plock[i] = dark_night(f).sum()\n",
    "#         print(i, dark_night(f).mean())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "conv = np.zeros(60)\n",
    "for i in range(60):  # extract night time for conversation dataset\n",
    "    f = f'./Inputs/sensing/conversation/conversation_u{i:02}.csv'\n",
    "    try:\n",
    "        conv[i] = dark_night(f).sum()\n",
    "#         print(i, dark_night(f).mean())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "phoneChar = np.zeros(60)\n",
    "for i in range(60):  # extract night time for phonecharge dataset\n",
    "    f = f'./Inputs/sensing/phonecharge/phonecharge_u{i:02}.csv'\n",
    "    try:\n",
    "        phoneChar[i] = dark_night(f).sum()\n",
    "#         print(i, dark_night(f).mean())\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bluetooth\n",
    "def transform(time_stamp): # transform timestamp into local time and return hour\n",
    "    utc_time = datetime.utcfromtimestamp(time_stamp)\n",
    "    time = utc_time + timedelta(hours = -4)\n",
    "\n",
    "    return time.hour\n",
    "\n",
    "def count_bluetooth(f):\n",
    "    file = open(f)\n",
    "    x = np.array([i for i in csv.reader(file) if i][1:])[:, 0].astype(int)\n",
    "    \n",
    "    g = np.array(list((transform(i) for i in x))).astype(int).reshape(-1,1)\n",
    "    \n",
    "    day = g[np.where((g[:, 0] > 8) * (g[:, 0] <= 18))].shape[0]\n",
    "    night = g[np.where(~((g[:, 0] > 8) * (g[:, 0] <= 18)))].shape[0]\n",
    "    \n",
    "    return np.array([day, night])\n",
    "\n",
    "bluetooth = np.zeros((60, 2))\n",
    "\n",
    "for i in range(60): # extract number of devices in daytime and nighttime\n",
    "    f = f'./Inputs/sensing/bluetooth/bt_u{i:02}.csv'\n",
    "    try:\n",
    "        bluetooth[i] = count_bluetooth(f)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the frequency in the daytime and nighttime\n",
    "def count_audio(f):\n",
    "    file = open(f)\n",
    "    x = np.array([i for i in csv.reader(file) if i][1:]).astype(int)\n",
    "\n",
    "    g = np.array(list((transform(i) for i in x[:,0]))).astype(int) # only consider hour\n",
    "    l = x[:,1] # audio label\n",
    "    g = np.stack((g, l), axis = 1)\n",
    "    day = g[np.where((g[:, 0] > 8) * (g[:, 0] <= 18))] # daytime 8-18\n",
    "    day_s = day[day[:, 1] == 0].shape[0] # silence, daytime\n",
    "    day_v = day[day[:, 1] == 1].shape[0] # voice, daytime\n",
    "    day_n = day[day[:, 1] == 2].shape[0] # noise, daytime\n",
    "    \n",
    "    night = g[np.where(~((g[:, 0] > 8) * (g[:, 0] <= 18)))] # nighttime 0-8 and 18-24\n",
    "\n",
    "    night_s = night[night[:,1] == 0].shape[0] # silence, nighttime\n",
    "    night_v = night[night[:,1] == 1].shape[0] # voice, nighttime\n",
    "    night_n = night[night[:,1] == 2].shape[0] # noise, nighttime\n",
    "    \n",
    "    return np.array([day_s, day_v, day_n, night_s, night_v, night_n])\n",
    "        \n",
    "audio = np.zeros((60,6))\n",
    "\n",
    "for i in range(60): # extract audio data splited by time\n",
    "    f = f'./Inputs/sensing/audio/audio_u{i:02}.csv'\n",
    "    try:\n",
    "        audio[i] = count_audio(f)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "activity = np.zeros((60,6)) # extract activity data splited by time\n",
    "for i in range(60):\n",
    "    f = f'./Inputs/sensing/activity/activity_u{i:02}.csv'\n",
    "    try:\n",
    "        activity[i] = count_audio(f)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "def norm(myData):\n",
    "    return (myData[:, 1:-1] - myData[:, 1:-1].min(axis = 0)) \\\n",
    "           / (myData[:, 1:-1].max(axis = 0) - myData[:, 1:-1].min(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine features\n",
    "def combine_feature(audio, activity, bluetooth, darkNights, plock, conv, phoneChar, label):\n",
    "    aud = np.array([audio[int(i[0])] for i in label]) # day and night, 0,1,2\n",
    "    act = np.array([activity[int(i[0])] for i in label]) # day and night, 0,1,2\n",
    "    blt = np.array([bluetooth[int(i[0])] for i in label]) # day and night\n",
    "    dk = np.array([darkNights[int(i[0])] for i in label]) # night duration\n",
    "    lk = np.array([plock[int(i[0])] for i in label]) # night duration\n",
    "    cv = np.array([conv[int(i[0])] for i in label]) # night duration\n",
    "    pcharge = np.array([phoneChar[int(i[0])] for i in label]) # night duration\n",
    "    conversation = get_data('conversation', label) # 10 weeks\n",
    "    activity_stationary = group_split('activity', 0, label) # 10 weeks\n",
    "    activity_walk = group_split('activity', 1, label) # 10 weeks\n",
    "    activity_run = group_split('activity', 2, label) # 10 weeks\n",
    "    audio_s = group_split('audio', 0, label) # 10 weeks\n",
    "    phonecharge = get_data('phonecharge', label) # 10 weeks\n",
    "    time_split_data = np.concatenate((label[:,0].reshape(-1,1), dk.reshape(-1,1)), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data,lk.reshape(-1,1)), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, cv.reshape(-1,1)), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, blt), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, aud), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, act), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, pcharge.reshape(-1,1)), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, phonecharge[:,1:-1]),axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, conversation[:, 1:-1]), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, activity_stationary[:, 1:-1]), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, activity_walk[:,1:-1]), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, activity_run[:,1:-1]), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, audio_s[:,1:-1]), axis = 1)\n",
    "    time_split_data = np.concatenate((time_split_data, label[:,1].reshape(-1,1)), axis = 1)\n",
    "    return time_split_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 78 features\n",
    "x_label = ['dark_night', 'phonelock_night', 'conversation_night', 'bluetooth_day', 'bluetooth_night', 'audio_0_day', \\\n",
    "          'audio_1_day', 'audio_2_day', 'audio_0_night', 'audio_1_night', 'audio_2_night', 'act_0_day', \\\n",
    "          'act_1_day', 'act_2_day', 'act_0_night', 'act_1_night', 'act_2_night', 'phonecharge_night', 'phonecharge_week1',\\\n",
    "          'phonecharge_week2', 'phonecharge_week3','phonecharge_week4','phonecharge_week5','phonecharge_week6','phonecharge_week7',\\\n",
    "          'phonecharge_week8','phonecharge_week9','phonecharge_week10','conversation_week1',\\\n",
    "          'conversation_week2', 'conversation_week3','conversation_week4','conversation_week5','conversation_week6','conversation_week7',\\\n",
    "          'conversation_week8','conversation_week9','conversation_week10','act0_week1',\\\n",
    "          'act0_week2', 'act0_week3','act0_week4','act0_week5','act0_week6','act0_week7',\\\n",
    "          'act0_week8','act0_week9','act0_week10','act1_week1',\\\n",
    "          'act1_week2', 'act1_week3','act1_week4','act1_week5','act1_week6','act1_week7',\\\n",
    "          'act1_week8','act1_week9','act1_week10','act2_week1',\\\n",
    "          'act2_week2', 'act2_week3','act0_week4','act2_week5','act2_week6','act2_week7',\\\n",
    "          'act2_week8','act2_week9','act2_week10','audio0_week1',\\\n",
    "          'audio0_week2', 'audio0_week3','audio0_week4','audio0_week5','audio0_week6','audio0_week7',\\\n",
    "          'audio0_week8','audio0_week9','audio0_week10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random forest\n",
    "def rfc_clf(x_train, y_train):\n",
    "    def  decorator(x_train, y_train):\n",
    "        model = RandomForestClassifier(random_state=0)\n",
    "        parameter = {'n_estimators':range(10,101,10),\\\n",
    "                     'max_depth':[1,2,3,4]}\n",
    "        clf=GridSearchCV(model,parameter,scoring='roc_auc',iid=True,cv=9)\n",
    "        clf.fit(x_train, y_train)\n",
    "        best_parameter = clf.best_params_\n",
    "        print('random forest best_parameter',best_parameter)\n",
    "        model = RandomForestClassifier(random_state=0,\\\n",
    "                                       n_estimators=best_parameter['n_estimators'],\\\n",
    "                                       max_depth=best_parameter['max_depth'])\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "    model = decorator(x_train,y_train)\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    "\n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    "    warnings.filterwarnings('ignore') \n",
    "    return report\n",
    "    \n",
    "# svm\n",
    "def svm_clf(x_train, y_train):\n",
    "    def decorator(x_train, y_train):\n",
    "        model = svm.SVC(random_state=0,gamma='auto')\n",
    "        parameter = {'kernel':['rbf','sigmoid','linear'],\\\n",
    "                     'degree':[1,2,3,4]}\n",
    "        clf=GridSearchCV(model,parameter,scoring='roc_auc',iid=True,cv=9)\n",
    "        clf.fit(x_train, y_train)\n",
    "        best_parameter = clf.best_params_\n",
    "        print('svm best_parameter',best_parameter)\n",
    "#         # print(best_parameter)\n",
    "        model = svm.SVC(random_state=0,gamma='auto',\\\n",
    "                        kernel=best_parameter['kernel'],\\\n",
    "                        degree=best_parameter['degree'])\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    model = decorator(x_train,y_train)\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    "\n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    "\n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    return report\n",
    "\n",
    "# knn\n",
    "def knn_clf(x_train, y_train):\n",
    "    def decorator(x_train, y_train):\n",
    "        model = KNeighborsClassifier()\n",
    "        parameter = {'n_neighbors':range(3,6),\\\n",
    "                     'algorithm':['auto','ball_tree','brute']}\n",
    "        clf=GridSearchCV(model,parameter,scoring='roc_auc',iid=True,cv=9)\n",
    "        clf.fit(x_train, y_train)\n",
    "        best_parameter = clf.best_params_\n",
    "        print('knn best_parameter',best_parameter)\n",
    "#         # print(best_parameter)\n",
    "        model = KNeighborsClassifier(\\\n",
    "                        n_neighbors=best_parameter['n_neighbors'],\\\n",
    "                        algorithm=best_parameter['algorithm'])\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "    warnings.filterwarnings('ignore') \n",
    "\n",
    "    model = decorator(x_train,y_train)\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    " \n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    " \n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# LogisticRegression\n",
    "def lr_(x_train, y_train):\n",
    "    def decorator(x_train, y_train):\n",
    "        model = LogisticRegression(solver='liblinear',random_state=0)\n",
    "        parameter = {'penalty':['l1','l2'],\\\n",
    "                     'class_weight':[None,{1:0.5, 0:0.5},{1:0.6, 0:0.4},{1:0.4, 0:0.6}]}\n",
    "        clf=GridSearchCV(model,parameter,scoring='roc_auc',iid=True,cv=9)\n",
    "        clf.fit(x_train, y_train)\n",
    "        best_parameter = clf.best_params_\n",
    "        print('lr best_parameter',best_parameter)\n",
    "#         # print(best_parameter)\n",
    "        model = LogisticRegression(solver='liblinear',\\\n",
    "                        penalty=best_parameter['penalty'],\\\n",
    "                        class_weight=best_parameter['class_weight'])\n",
    "        model.fit(x_train, y_train)\n",
    "        return model\n",
    "\n",
    "\n",
    "    model = decorator(x_train,y_train)\n",
    "#     y_pre = model.predict(x_train)\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    "\n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    "\n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    return report    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain evaluation matrix\n",
    "def collect_socre(x_train,y_train):\n",
    "    score_matrix=np.array(( \n",
    "            rfc_clf(x_train, y_train),\n",
    "            lr_(x_train, y_train),\n",
    "            knn_clf(x_train, y_train),\n",
    "            svm_clf(x_train, y_train) ))\n",
    "    df = pd.DataFrame(data=score_matrix,columns=['auc_roc','accuracy','precision','recall','f1_score'],index=['rfc','lr','knn','svm'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "class DenseModel(torch.nn.Module):\n",
    "    def __init__(self, num_of_features):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(num_of_features, 64)\n",
    "        self.l2 = torch.nn.Linear(64, 64)\n",
    "        \n",
    "        self.l3 = torch.nn.Linear(64,128)\n",
    "        self.l4 = torch.nn.Linear(128,128)\n",
    "        \n",
    "        self.l5 = torch.nn.Linear(64 + 128, 256)\n",
    "        self.l6 = torch.nn.Linear(256, 256)\n",
    "    \n",
    "        self.l7 = torch.nn.Linear(64 + 128 + 256, 512)\n",
    "        self.l8 = torch.nn.Linear(512, 512)\n",
    "        \n",
    "        self.end = torch.nn.Linear(512, 2)\n",
    "        self.do = torch.nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x1 = self.do(torch.relu(self.l1(x))) # (features - 64)\n",
    "        x2 = self.do(torch.relu(self.l2(x1))) # (64 - 64)\n",
    "        x3 = self.do(torch.relu(self.l2(x2))) # (64 - 64)\n",
    "        \n",
    "        x4 = self.do(torch.relu(self.l3(x3))) # (64 - 128)\n",
    "        x5 = self.do(torch.relu(self.l4(x4))) # (128 - 128)\n",
    "        x6 = self.do(torch.relu(self.l4(x5))) # (128 - 128)\n",
    "        \n",
    "        x6c = torch.cat((x3, x6), dim = 1)\n",
    "        x7 = self.do(torch.relu(self.l5(x6c))) # (192 - 256)\n",
    "        x8 = self.do(torch.relu(self.l6(x7))) # (256 - 256)\n",
    "        x9 = self.do(torch.relu(self.l6(x8))) # (256 - 256)\n",
    "        \n",
    "        x9c = torch.cat((x3, x6, x9), dim = 1)\n",
    "        x10 = self.do(torch.relu(self.l7(x9c))) # (448 - 512)\n",
    "        x11 = self.do(torch.relu(self.l8(x10))) # (512 - 512)\n",
    "        x12 = self.do(torch.relu(self.l8(x11))) # (512 - 512)\n",
    "        \n",
    "        return self.end(x12) # (512 - 2)\n",
    "        \n",
    "# def metric(output, target):\n",
    "def metric(tp, tn, fp, fn):\n",
    "    metrics = {'recall': 0,\n",
    "               'precision': 0,\n",
    "               'accuracy': 0,\n",
    "               'f1score': 0}\n",
    "    print(tp, tn, fp, fn)\n",
    "    metrics['recall'] = tp/(tp + fn)\n",
    "    metrics['precision'] = tp/(tp + fp)\n",
    "    metrics['accuracy'] = (tp + tn)/(tp + tn + fp + fn)\n",
    "    metrics['f1score'] = 2 * metrics['recall'] * metrics['precision']/(metrics['recall'] + metrics['precision'])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network with augmentation\n",
    "def nn_aug(x_train, y_train):\n",
    "    to_aug_x = np.array(x_train)\n",
    "    to_aug_y = np.array(y_train)\n",
    "    print(to_aug_x.shape, to_aug_y.shape)\n",
    "    to_aug_data = np.concatenate((to_aug_x, to_aug_y.reshape(-1,1)), axis = 1)\n",
    "    to_aug_data = np.repeat(to_aug_data, 100, axis = 0)\n",
    "    np.random.shuffle(to_aug_data)\n",
    "\n",
    "    aug_x = np.zeros((100 * x_train.shape[0], 10))\n",
    "    aug_y = np.zeros((100 * x_train.shape[0],1))\n",
    "    aug_x = to_aug_data[:,:10]\n",
    "    aug_y = to_aug_data[:,-1]\n",
    "\n",
    "    for i in range(100 * x_train.shape[0]):\n",
    "        aug_x[i,:] += np.random.normal(0,0.33,10).round(2)\n",
    "\n",
    "    train_x = torch.tensor(aug_x)\n",
    "    train_y = torch.tensor(aug_y)\n",
    "    dataset = torch.tensor(x_train)\n",
    "    target = torch.tensor(y_train)\n",
    "    bin_target = torch.zeros(x_train.shape[0],2)\n",
    "    bin_target[(target == 0).flatten(), 0] = 1\n",
    "    bin_target[(target == 1).flatten(), 1] = 1\n",
    "    train_target = torch.zeros(100 * x_train.shape[0],2)\n",
    "    train_target[(train_y == 0).flatten(), 0] = 1\n",
    "    train_target[(train_y == 1).flatten(), 1] = 1\n",
    "\n",
    "    trainTensors = torch.utils.data.TensorDataset(train_x, train_target)\n",
    "    testTensors = torch.utils.data.TensorDataset(dataset, bin_target)\n",
    "\n",
    "    trainingLoader = torch.utils.data.DataLoader(trainTensors, batch_size = 10, shuffle = True)\n",
    "    testingLoader = torch.utils.data.DataLoader(testTensors, shuffle = True)\n",
    "    model = DenseModel(10)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.3)\n",
    "    lossfn = F.binary_cross_entropy_with_logits\n",
    "\n",
    "    epoch = 30\n",
    "\n",
    "    trainLoss = np.zeros(epoch)\n",
    "    valLoss = np.zeros(epoch)\n",
    "    since = time.time()\n",
    "    for i in range(1, epoch + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "    #     print(lr_scheduler.get_lr())\n",
    "        epoch_size = 0\n",
    "        for data, target in trainingLoader:\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(data.float())\n",
    "    #         print(output)\n",
    "            loss = lossfn(output, target)\n",
    "    #         print(loss, target)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_size += data.size(0)\n",
    "        lr_scheduler.step()\n",
    "        trainLoss[i - 1] = epoch_loss/epoch_size\n",
    "        print(i)\n",
    "    print(f\"{int(time.time() - since)}s used!\")\n",
    "\n",
    "    a, = plt.plot(np.arange(0,30), trainLoss, c = 'blue', label = 'train')\n",
    "    plt.show()\n",
    "    model.eval()\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    y_score = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in testingLoader:\n",
    "            output = model(data.float())\n",
    "    #             print(output)\n",
    "    #             print(torch.sigmoid(output))\n",
    "            y_score.append(torch.sigmoid(output).flatten()[1].cpu())\n",
    "            pred = torch.sigmoid(output).argmax()\n",
    "    #         print(pred)\n",
    "            target = target.flatten().argmax()\n",
    "            y_true.append(target.cpu())\n",
    "            print(pred, target)\n",
    "            if pred >= 0.5 and target == 1:\n",
    "                tp += 1\n",
    "            elif pred >= 0.5 and target == 0:\n",
    "                fp += 1\n",
    "            elif pred < 0.5 and target == 0:\n",
    "                tn += 1\n",
    "            elif pred < 0.5 and target == 1:\n",
    "                fn += 1\n",
    "    metrics = metric(tp, tn, fp, fn)\n",
    "    print(metrics)\n",
    "    print(roc_auc_score(np.array(y_true), np.array(y_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network without augmentation\n",
    "def nn(x_train, y_train):\n",
    "    temp2 = np.zeros((5,5))\n",
    "    for cv in range(5):\n",
    "        dataset = torch.tensor(x_train)\n",
    "        target = torch.tensor(y_train)\n",
    "        bin_target = torch.zeros(x_train.shape[0],2)\n",
    "        bin_target[(target == 0).flatten(), 0] = 1\n",
    "        bin_target[(target == 1).flatten(), 1] = 1\n",
    "\n",
    "        trainTensors = torch.utils.data.TensorDataset(dataset[:25], bin_target[:25])\n",
    "        testTensors = torch.utils.data.TensorDataset(dataset[25:], bin_target[25:])\n",
    "\n",
    "        trainingLoader = torch.utils.data.DataLoader(trainTensors, shuffle = True)\n",
    "        testingLoader = torch.utils.data.DataLoader(testTensors, shuffle = True)\n",
    "        model = DenseModel(10)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.3)\n",
    "        lossfn = F.binary_cross_entropy_with_logits\n",
    "\n",
    "        epoch = 200\n",
    "\n",
    "        trainLoss = np.zeros(epoch)\n",
    "        valLoss = np.zeros(epoch)\n",
    "        since = time.time()\n",
    "        for i in range(1, epoch + 1):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "        #     print(lr_scheduler.get_lr())\n",
    "            epoch_size = 0\n",
    "            for data, target in trainingLoader:\n",
    "                model.zero_grad()\n",
    "\n",
    "                output = model(data.float())\n",
    "        #         print(output)\n",
    "                loss = lossfn(output, target)\n",
    "        #         print(loss, target)\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_size += data.size(0)\n",
    "            lr_scheduler.step()\n",
    "            trainLoss[i - 1] = epoch_loss/epoch_size\n",
    "            model.eval()\n",
    "\n",
    "        print(f\"{int(time.time() - since)}s used!\")\n",
    "\n",
    "        a, = plt.plot(np.arange(0,200), trainLoss, c = 'blue', label = 'train')\n",
    "        plt.show()\n",
    "        model.eval()\n",
    "        tp, tn, fp, fn = 0, 0, 0, 0\n",
    "        y_score = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in testingLoader:\n",
    "                output = model(data.float())\n",
    "    #             print(output)\n",
    "    #             print(torch.sigmoid(output))\n",
    "                y_score.append(torch.sigmoid(output).flatten()[1].cpu())\n",
    "                pred = torch.sigmoid(output).argmax()\n",
    "        #         print(pred)\n",
    "                target = target.flatten().argmax()\n",
    "                y_true.append(target.cpu())\n",
    "    #             print(pred, target)\n",
    "                if pred >= 0.5 and target == 1:\n",
    "                    tp += 1\n",
    "                elif pred >= 0.5 and target == 0:\n",
    "                    fp += 1\n",
    "                elif pred < 0.5 and target == 0:\n",
    "                    tn += 1\n",
    "                elif pred < 0.5 and target == 1:\n",
    "                    fn += 1\n",
    "        if (tp + fp) == 0:\n",
    "            metrics = {'recall':tp/(tp + fn), 'precision':0, 'accuracy':(tp + tn)/(tp + tn + fp + fn), 'f1-score':0}\n",
    "        else:\n",
    "            metrics = metric(tp, tn, fp, fn)\n",
    "        print(metrics)\n",
    "        for j,met in enumerate(metrics):\n",
    "            temp2[cv, j] = metrics[met]\n",
    "        temp2[cv, -1] = roc_auc_score(np.array(y_true), np.array(y_score))\n",
    "        print(roc_auc_score(np.array(y_true), np.array(y_score)))\n",
    "        print(temp2[~(temp2[:,0] == 0)].mean(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flourishing post tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post\n",
    "pre_post = 'post'\n",
    "\n",
    "label2 = get_flour(pre_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 78 features\n",
    "data = combine_feature(audio, activity, bluetooth, darkNights, plock, conv, phoneChar, label2)\n",
    "normData = norm(data) # normalization\n",
    "y = data[:, -1]\n",
    "x_train = normData\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "collect_socre(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing features with low variance\n",
    "sel = feature_selection.VarianceThreshold(0)\n",
    "train_variance = sel.fit_transform(x_train)\n",
    "train_variance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlations to target\n",
    "df=np.concatenate((x_train,data[:,-1].reshape(-1,1)),axis = 1)\n",
    "print(df.shape)\n",
    "a=[str(i) for i in range(1,79)]\n",
    "a.append('target')\n",
    "df = pd.DataFrame(df,columns=a)\n",
    "\n",
    "corr_matrix = df.corr().abs()\n",
    "print(corr_matrix['target'].sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select upper triangle of correlation matrix\n",
    "matrix = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with high correlation\n",
    "to_drop = [column for column in matrix.columns if any(matrix[column] > 0.8)]\n",
    "print('Columns to drop: ' , (len(to_drop)))\n",
    "print(to_drop)\n",
    "new=df.drop(to_drop,axis=1)\n",
    "new=new.to_numpy()\n",
    "new_x_train=new[:,:-1]\n",
    "new_y_train=new[:,-1]\n",
    "\n",
    "# drop 45 features\n",
    "# feature extraction\n",
    "k_best = feature_selection.SelectKBest(score_func=feature_selection.f_classif, k=20)\n",
    "# fit and transform\n",
    "new_x_train = k_best.fit_transform(new_x_train,y_train)\n",
    "print(new_x_train.shape)\n",
    "print(collect_socre(new_x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the corresponding index\n",
    "new_features = []\n",
    "new_feature_label = []\n",
    "for i in range(len(x_train.T)):\n",
    "    for k in range(10):\n",
    "        if list(x_train[:,i]) == list(new_x_train[:,k]):\n",
    "            new_features.append(i)\n",
    "            new_feature_label.append(x_label[i])\n",
    "            print(i, x_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kbest\n",
    "for n in range(10,11):\n",
    "    # feature extraction\n",
    "    k_best = feature_selection.SelectKBest(score_func=feature_selection.f_classif, k=n)\n",
    "    # fit and transform\n",
    "    new_x_train = k_best.fit_transform(x_train,y_train)\n",
    "    print(new_x_train.shape)\n",
    "    print(collect_socre(new_x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the corresponding index\n",
    "new_features = []\n",
    "new_feature_label = []\n",
    "for i in range(len(x_train.T)):\n",
    "    for k in range(10):\n",
    "        if list(x_train[:,i]) == list(new_x_train[:,k]):\n",
    "            new_features.append(i)\n",
    "            new_feature_label.append(x_label[i])\n",
    "            print(i, x_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the top 10 features with the highest correlation\n",
    "# feature extraction\n",
    "# evaluation matrix for top 10 features\n",
    "for n in range(10,11):\n",
    "    rfe = feature_selection.RFE(RandomForestClassifier(n_estimators=1000,random_state=0), n_features_to_select=n)\n",
    "    new_x_train = rfe.fit_transform(x_train, y_train)\n",
    "    print(new_x_train.shape)\n",
    "    print(collect_socre(new_x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the corresponding index\n",
    "new_features = []\n",
    "new_feature_label = []\n",
    "for i in range(len(x_train.T)):\n",
    "    for k in range(10):\n",
    "        if list(x_train[:,i]) == list(new_x_train[:,k]):\n",
    "            new_features.append(i)\n",
    "            new_feature_label.append(x_label[i])\n",
    "            print(i, x_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlations to target\n",
    "df=np.concatenate((new_x_train,data[:,-1].reshape(-1,1)),axis = 1)\n",
    "print(df.shape)\n",
    "a=[str(i) for i in range(1,11)]\n",
    "a.append('target')\n",
    "df = pd.DataFrame(df,columns=a)\n",
    "\n",
    "corr_matrix = df.corr().abs()\n",
    "print(corr_matrix['target'].sort_values(ascending=False).head(10))\n",
    "sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select 10 features and apply to other measures \n",
    "\n",
    "[36, 43, 51, 56, 58, 66, 67, 69, 71, 73]\n",
    "['conversation_week9', 'act0_week6', 'act1_week4', 'act1_week9', 'act2_week1', 'act2_week9', 'act2_week10', 'audio0_week2', 'audio0_week4', 'audio0_week6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_features)\n",
    "print(new_feature_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neural network with augmentation\n",
    "# on top 10 features\n",
    "nn_aug(x_train[:,new_features], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network without augmentation\n",
    "nn(x_train[:,new_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pos_label post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post\n",
    "pre_post = 'post'\n",
    "\n",
    "pos_label, neg_label = get_panas(pre_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract best features\n",
    "def extract_feature(dataset, features):\n",
    "    new_data = np.zeros((dataset.shape[0], 10))\n",
    "\n",
    "    for j in range(len(new_features)):\n",
    "        new_data[:,j] = dataset[:, features[j]]\n",
    "    new_data = np.concatenate((dataset[:,0].reshape(-1,1), new_data), axis = 1)\n",
    "    new_data = np.concatenate((new_data, dataset[:,-1].reshape(-1,1)), axis = 1)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_feature(audio, activity, bluetooth, darkNights, plock, conv, phoneChar, pos_label)\n",
    "\n",
    "data = extract_feature(data, new_features)\n",
    "normData = norm(data) # normalization\n",
    "\n",
    "y = data[:, -1]\n",
    "x_train = normData\n",
    "y_train = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing data\n",
    "# random forest\n",
    "def rfc_clf(x_train, y_train):\n",
    "    model = RandomForestClassifier(random_state=0,max_depth=3,n_estimators=90)\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    "    warnings.filterwarnings('ignore') \n",
    "    return report\n",
    "    \n",
    "# svm\n",
    "def svm_clf(x_train, y_train):\n",
    "    model = svm.SVC(random_state=0,gamma='auto',degree=1,kernel='linear')\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    return report\n",
    "\n",
    "# knn\n",
    "def knn_clf(x_train, y_train):\n",
    "    model = KNeighborsClassifier(algorithm='auto',n_neighbors=3)\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# LogisticRegression\n",
    "def lr_(x_train, y_train):\n",
    "    model = LogisticRegression(solver='liblinear',random_state=0,class_weight={1: 0.6, 0: 0.4},penalty='l2')\n",
    "    score = cross_val_score(model,x_train,y_train,cv=9,scoring='roc_auc').mean()\n",
    "    accuracy = cross_val_score(model,x_train,y_train,cv=9,scoring='accuracy').mean()\n",
    "    precision = cross_val_score(model,x_train,y_train,cv=9,scoring='precision').mean()\n",
    "    recall = cross_val_score(model,x_train,y_train,cv=9,scoring='recall').mean()\n",
    "    f1_score = cross_val_score(model,x_train,y_train,cv=9,scoring='f1').mean()\n",
    "    report = [score,accuracy,precision,recall,f1_score]\n",
    "    warnings.filterwarnings('ignore') \n",
    "    \n",
    "    return report   \n",
    "\n",
    "def collect_socre(x_train,y_train):\n",
    "    score_matrix=np.array(( \n",
    "            rfc_clf(x_train, y_train),\n",
    "            lr_(x_train, y_train),\n",
    "            knn_clf(x_train, y_train),\n",
    "            svm_clf(x_train, y_train) ))\n",
    "    df = pd.DataFrame(data=score_matrix,columns=['auc_roc','accuracy','precision','recall','f1_score'],index=['rfc','lr','knn','svm'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collect_socre(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neural network with augmentation\n",
    "nn_aug(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network without augmentation\n",
    "nn(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neg_label post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_feature(audio, activity, bluetooth, darkNights, plock, conv, phoneChar, neg_label)\n",
    "data = extract_feature(data, new_features)\n",
    "normData = norm(data) # normalization\n",
    "y = data[:, -1]\n",
    "x_train = normData\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collect_socre(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neural network with augmentation\n",
    "nn_aug(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neural network without augmentation\n",
    "nn(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flourishing pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_post = 'pre'\n",
    "\n",
    "label2 = get_flour(pre_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_feature(audio, activity, bluetooth, darkNights, plock, conv, phoneChar, label2)\n",
    "data = extract_feature(data, new_features)\n",
    "normData = norm(data) # normalization\n",
    "y = data[:, -1]\n",
    "x_train = normData\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collect_socre(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neural network with augmentation\n",
    "nn_aug(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network without augmentation\n",
    "nn(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flourishing pre and post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre and post\n",
    "label2 = get_flour_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_feature(audio, activity, bluetooth, darkNights, plock, conv, phoneChar, label2)\n",
    "data = extract_feature(data, new_features)\n",
    "normData = norm(data) # normalization\n",
    "y = data[:, -1]\n",
    "x_train = normData\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collect_socre(x_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# neural network with augmentation\n",
    "nn_aug(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network without augmentation\n",
    "nn(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
